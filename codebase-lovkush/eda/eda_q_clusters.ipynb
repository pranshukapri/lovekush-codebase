{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import textwrap # for wrapping text for plotly charts\n",
    "\n",
    "from sklearn.feature_extraction import text #to access stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from bokeh.plotting import figure, output_notebook, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool, CategoricalColorMapper, LinearColorMapper\n",
    "from bokeh.palettes import d3, Magma256\n",
    "output_notebook()\n",
    "from bokeh.io import save\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"/project/data_preprocessed_csv/\"\n",
    "dir_pickle = \"/project/pickled_vectors/\"\n",
    "df_metadata = pd.read_csv(\"/project/preprocessing/metadata.csv\", index_col=0)\n",
    "\n",
    "\n",
    "class QuestionClusterizer:\n",
    "    def __init__(self, vectorizer, dim_reducer=umap.UMAP, random_state = 0):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.filenames = None\n",
    "        self.corpus = None\n",
    "        self.df_metadata = pd.read_csv(\n",
    "            \"/project/preprocessing/metadata.csv\", index_col=0\n",
    "        )\n",
    "        self.dim_reducer = dim_reducer\n",
    "        self.random_state = random_state\n",
    "        self.title = \"Visualising questions from a set of depositions\"\n",
    "        self.dir_pickle = \"/project/pickled_vectors/\"\n",
    "\n",
    "    def create_filenames_from_metadata(self):\n",
    "        # i only include cases with multiple depositions\n",
    "        # to change this, change the 1 to a 0 in the second line\n",
    "        counts = self.df_metadata.groupby(\"case\").case.count()\n",
    "        cases = counts[counts > 1].index.values\n",
    "        \n",
    "        for i, case in enumerate(cases):\n",
    "            print(f\"{i:2}: {case}\")\n",
    "        \n",
    "        case_selection = int(input(\"Select a case by providing the number: \"))\n",
    "        \n",
    "        self.filenames = self.df_metadata.loc[\n",
    "            self.df_metadata.case == cases[case_selection], \"filename\"\n",
    "        ].values\n",
    "\n",
    "    def create_corpus(self):\n",
    "        self.corpus = []\n",
    "        for filename in self.filenames:\n",
    "            df = pd.read_csv(dir_name + filename[:-3] + \"csv\", index_col=0)\n",
    "            df = df[df.text_type.isin([\"q\"])]\n",
    "            try:\n",
    "                self.corpus += df.text.values.tolist()\n",
    "            except:\n",
    "                print(filename)\n",
    "\n",
    "    def vectorize(self):\n",
    "        # do over-simplistic check for universal sentence embedding vectoriser\n",
    "        if \"tensorflow\" in str(type(self.vectorizer)):\n",
    "            self.vectors = self.vectorizer(self.corpus)\n",
    "        else:\n",
    "            # else assume we have vectoriser with fit_transform method\n",
    "            self.vectors = self.vectorizer.fit_transform(self.corpus).toarray()\n",
    "\n",
    "    def reduce_dimensions(self):\n",
    "        self.vectors_dim_reduced = self.dim_reducer(\n",
    "            random_state=self.random_state\n",
    "        ).fit_transform(self.vectors)\n",
    "\n",
    "    def clusterize(self):\n",
    "        hdbscan_clusterer = hdbscan.HDBSCAN()\n",
    "        hdbscan_clusterer.fit(self.vectors_dim_reduced)\n",
    "        self.clusters = hdbscan_clusterer.labels_\n",
    "\n",
    "    def score_clusters(self):\n",
    "        # hdbscan has label of -1 for points it considers to be 'noise' or not a part of a cluster\n",
    "        # these have been removed for purposes of calculating silhouette.\n",
    "        # to include noise, just remove the `> -1`\n",
    "        indices = self.clusters > -1\n",
    "        self.silhouette = silhouette_score(\n",
    "            self.vectors_dim_reduced[indices], self.clusters[indices]\n",
    "        )\n",
    "\n",
    "    def plot_plotly(self, clusters=True, remove_noise = True):\n",
    "        if not clusters:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        df_plot = pd.DataFrame(self.vectors_dim_reduced, columns=[\"x\", \"y\"])\n",
    "        df_plot[\"Text\"] = self.corpus\n",
    "        df_plot[\"cluster\"] = self.clusters\n",
    "        \n",
    "        # need to manually wrap text for plotly graphs, using html newline tags\n",
    "        df_plot.Text = df_plot.Text.apply(\n",
    "            lambda txt: \"<br>\".join(textwrap.wrap(txt, width=30))\n",
    "        )\n",
    "        \n",
    "        if remove_noise:\n",
    "            df_plot = df_plot[df_plot.cluster > -1]\n",
    "\n",
    "        fig = px.scatter(\n",
    "            df_plot,\n",
    "            x=\"x\",\n",
    "            y=\"y\",\n",
    "            hover_data=dict(x=False, y=False, Text=True, cluster=False),\n",
    "            width=600,\n",
    "            height=600,\n",
    "            color=\"cluster\",\n",
    "            color_continuous_scale=\"rainbow\",\n",
    "        )\n",
    "\n",
    "        fig.update(layout_coloraxis_showscale=False)\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor=\"rgba(0, 0, 0, 0)\",\n",
    "            title=\"Visualising all questions from depositions of a certain case\",\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(\n",
    "            linecolor=\"black\",\n",
    "            mirror=True,\n",
    "            title=None,\n",
    "            showticklabels=False,\n",
    "            linewidth=2,\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            linecolor=\"black\",\n",
    "            mirror=True,\n",
    "            title=None,\n",
    "            showticklabels=False,\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    def plot_bokeh(self, clusters=True, remove_noise = True):\n",
    "        if not clusters:\n",
    "            raise NotImplementedError\n",
    "\n",
    "            \n",
    "        df_plot = pd.DataFrame(self.vectors_dim_reduced, columns=[\"x\", \"y\"])\n",
    "        df_plot[\"text\"] = self.corpus\n",
    "        df_plot[\"clusters\"] = self.clusters\n",
    "        \n",
    "        if remove_noise:\n",
    "            df_plot = df_plot[df_plot.clusters > -1]\n",
    "            \n",
    "        source = ColumnDataSource(\n",
    "            data=dict(\n",
    "                x=df_plot.x,\n",
    "                y=df_plot.y,\n",
    "                clusters=df_plot.clusters,\n",
    "                text=df_plot.text,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        color_map = LinearColorMapper(\n",
    "            palette=\"Turbo256\", low=self.clusters.min(), high=self.clusters.max()\n",
    "        )\n",
    "\n",
    "        TOOLS = \"box_zoom,hover,reset\"\n",
    "        p = figure(title=self.title + f\". Silhouette: {self.silhouette}\", tools=TOOLS)\n",
    "        p.background_fill_color = \"white\"\n",
    "        p.xgrid.grid_line_color = None\n",
    "        p.ygrid.grid_line_color = None\n",
    "\n",
    "        p.scatter(\n",
    "            x=\"x\",\n",
    "            y=\"y\",\n",
    "            color={\"field\": \"clusters\", \"transform\": color_map},\n",
    "            source=source,\n",
    "        )\n",
    "\n",
    "        hover = p.select(dict(type=HoverTool))\n",
    "        hover.tooltips = [\n",
    "            (\"text\", \"@text\"),\n",
    "        ]\n",
    "\n",
    "        show(p)\n",
    "\n",
    "    def fit(self, use_metadata=True, filenames=None):\n",
    "        if use_metadata:\n",
    "            self.create_filenames_from_metadata()\n",
    "        else:\n",
    "            self.filenames = filenames\n",
    "\n",
    "        self.create_corpus()\n",
    "        self.vectorize()\n",
    "        self.reduce_dimensions()\n",
    "        self.clusterize()\n",
    "        self.score_clusters()\n",
    "        self.plot_plotly()\n",
    "\n",
    "    def pickle_clusterizer(self, filename):\n",
    "        # note that the vectorizer and dim_reducer are not stored - only the name of them are\n",
    "        # stored. this is because pickle module cannot store them.\n",
    "        info = dict(\n",
    "            vectorizer=str(self.vectorizer),\n",
    "            dim_reducer=str(self.dim_reducer),\n",
    "            filenames=self.filenames,\n",
    "            corpus=self.corpus,\n",
    "            vectors=self.vectors,\n",
    "            vectors_dim_reduced=self.vectors_dim_reduced,\n",
    "            clusters=self.clusters,\n",
    "        )\n",
    "        with open(self.dir_pickle + filename, \"wb\") as f:\n",
    "            pickle.dump(info, f)\n",
    "\n",
    "    def load_pickle(self, filename):\n",
    "        with open(self.dir_pickle + filename, \"rb\") as f:\n",
    "            info = pickle.load(f)\n",
    "        self.vectorizer = info[\"vectorizer\"]\n",
    "        self.dim_reducer = info[\"dim_reducer\"]\n",
    "        self.filenames = info[\"filenames\"]\n",
    "        self.corpus = info[\"corpus\"]\n",
    "        self.vectors = info[\"vectors\"]\n",
    "        self.vectors_dim_reduced = info[\"vectors_dim_reduced\"]\n",
    "        self.clusters = info[\"clusters\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing clusterizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf = True,\n",
    "                            ngram_range = (2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterizer = QuestionClusterizer(vectorizer, dim_reducer=TSNE)\n",
    "clusterizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusterizer.pickle_clusterizer('tfidf22_fnone_snone_hdbscan_tsne_cell.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing universal sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterizer = QuestionClusterizer(vectorizer = embed)\n",
    "clusterizer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'use_hdbscan.pkl'\n",
    "# clusterizer.pickle_clusterizer(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
