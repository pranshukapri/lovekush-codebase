{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import hashlib  # for detecting duplicate\n",
    "from tqdm.notebook import tqdm  # for progress bars\n",
    "\n",
    "\n",
    "dir_name = \"../data_preprocessed/\"\n",
    "dir_name_csv = \"../data_preprocessed_csv/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## known issues with preprocessing\n",
    "\n",
    "1) Sometimes in a question, we have the questioner being identified:\\\n",
    "`Q.  (BY MR. SMITH) <question>`\\\n",
    "The `(BY MR. SMITH)` should be removed as it is not part of the question. Currently it is not being removed\n",
    "\n",
    "2) Certain types of headers are not being detected and are considered part of a question/answer. For example:\\\n",
    "`I have no further questions. F U R T H E R E X A M I N A T I O N`\n",
    "\n",
    "3) The page numbering does not always work correctly. One reason why is from the file `Transcript_for_sync_HG082219.txt`:\\\n",
    "`00010:01 Q. Okay. Did you look at the LLC agreement?`\\\n",
    "In most depositions, the page number has its own line at the top, whereas in this deposition, the page number and first line of text is part of top line of the page.\n",
    "\n",
    "4) There are certain kinds of duplicates that are not detected. I have not spent any time to work out why. E.g.:\\\n",
    "`82819_Sicilia_Saimesier.csv`, `82819_Sicilia_Saimesier(1).csv`\\\n",
    "`19-0813_Kelly_Boyle_Volume_2.txt`, `19-0813_Kelly_Boyle_Volume_2(1).txt`, `v19-0813_Kelly_Boyle_Volume_2(2).txt`\n",
    "\n",
    "5) Identifying questioner does not always work. E.g. in `082319.txt`:\\\n",
    "```9        capacity as Trustee.\n",
    "10    EXAMINATION BY\n",
    "11    MR. CROKE:\n",
    "12        Q.    Good morning, Mr. Mathur.```\\\n",
    "Currently I look for the phrase `BY MR. CROKE` on one line, whereas here it is split on two lines\n",
    "\n",
    "6) No attempt has been made to process all the preamable at the start. Dataframe only starts from 5 lines above the first question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_space_with_underscore():\n",
    "    for f in os.listdir(dir_name):\n",
    "        new_name = f.replace(\" \", \"_\")\n",
    "        os.rename(dir_name + f, dir_name + new_name)\n",
    "    for f in os.listdir(dir_name):\n",
    "        print(f)\n",
    "\n",
    "\n",
    "def strip_lines(lines):\n",
    "    \"\"\"\n",
    "    strip spaces, newlines, carriage returns from each of the lines\n",
    "    \"\"\"\n",
    "    lines_new = []\n",
    "\n",
    "    for line in lines:\n",
    "        lines_new.append(line.strip(\" \\n\\x0c\"))\n",
    "\n",
    "    return lines_new\n",
    "\n",
    "\n",
    "def delete_blank_lines(lines):\n",
    "    \"\"\"\n",
    "    given list of lines with \\n removed, deletes those lines that are empty\n",
    "    \"\"\"\n",
    "    lines_new = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            lines_new.append(line)\n",
    "\n",
    "    return lines_new\n",
    "\n",
    "\n",
    "def remove_time_tags(lines):\n",
    "    \"\"\"\n",
    "    given list of lines, remove time tags at the start or end of the lines\n",
    "    \"\"\"\n",
    "    # added requirement for 2 spaces in pat_end because it is possible a\n",
    "    # time is mentioned in a question or answer\n",
    "    # the ?: at the start of each group makes them non-capturing groups\n",
    "    timestamp = r\"(?:\\d\\d:\\d\\d(?::\\d\\d)?(?:[AP]M)?)\"\n",
    "    pat_start = \"^\" + timestamp + \" +(.*)\"\n",
    "    pat_end = \"(.*) {2,}\" + timestamp + \"$\"\n",
    "\n",
    "    lines_new = []\n",
    "\n",
    "    for line in lines:\n",
    "        match_start = re.match(pat_start, line)\n",
    "        match_end = re.match(pat_end, line)\n",
    "\n",
    "        if match_start:\n",
    "            lines_new.append(match_start.groups()[0])\n",
    "        elif match_end:\n",
    "            lines_new.append(match_end.groups()[0])\n",
    "        else:\n",
    "            lines_new.append(line)\n",
    "\n",
    "    return lines_new\n",
    "\n",
    "\n",
    "def is_start_of_question(text):\n",
    "    \"\"\"\n",
    "    determine if the text is the start of a question\n",
    "    \"\"\"\n",
    "    pat = r\"^Q[\\. ] +[^ ]\"\n",
    "\n",
    "    match = re.match(pat, text)\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_identifying_questioner(text):\n",
    "    \"\"\"\n",
    "    given text, determine if it is of the form:\n",
    "    BY MS. SMITH:\n",
    "    \"\"\"\n",
    "    pat = r\"B[Yy][ -](M[RrSs]\\. ?([A-Z][A-Za-z-]+\\b ?)+|([A-Z][A-Za-z-]+\\b ?)+(, ESQ))\"\n",
    "    match = re.search(pat, text)\n",
    "\n",
    "    if match:\n",
    "        return match.groups()[0].upper()\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def find_first_question(lines):\n",
    "    \"\"\"\n",
    "    find first question in deposition.\n",
    "    returns index from lines if found\n",
    "    \"\"\"\n",
    "\n",
    "    found = False\n",
    "    for i, line in enumerate(lines):\n",
    "        splits = split_into_num_text(line)\n",
    "        if splits is None:\n",
    "            continue\n",
    "\n",
    "        _, text = splits\n",
    "\n",
    "        if is_start_of_question(text):\n",
    "            found = i\n",
    "            break\n",
    "\n",
    "    return found\n",
    "\n",
    "\n",
    "def is_start_of_answer(text):\n",
    "    \"\"\"\n",
    "    determine if the text is the start of an answer\n",
    "    \"\"\"\n",
    "    pat = r\"^A[\\. ] +[^ ]\"\n",
    "\n",
    "    match = re.match(pat, text)\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_bad_files(dir_name):\n",
    "    \"\"\"\n",
    "    a file is considered bad if the function find_first_question returns false\n",
    "    manually checking suggests these files are not depositions\n",
    "    \"\"\"\n",
    "    bad_files = []\n",
    "\n",
    "    for filename in os.listdir(dir_name):\n",
    "        with open(dir_name + filename, \"r\", encoding=\"windows-1252\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        lines = strip_lines(lines)\n",
    "        lines = delete_blank_lines(lines)\n",
    "        lines = remove_time_tags(lines)\n",
    "\n",
    "        if not find_first_question(lines):\n",
    "            bad_files.append(filename)\n",
    "\n",
    "    return bad_files\n",
    "\n",
    "\n",
    "def is_page_number(line):\n",
    "    \"\"\"\n",
    "    given line from file, determine if corresponds to a page numbering\n",
    "    if yes, return the page number\n",
    "    if not, return 0\n",
    "    \"\"\"\n",
    "    pat = r\"0*(\\d+)$\"\n",
    "    match = re.match(pat, line)\n",
    "    if not match:\n",
    "        return 0\n",
    "    else:\n",
    "        return int(match.groups()[0])\n",
    "\n",
    "\n",
    "def find_current_page_number(lines):\n",
    "    \"\"\"\n",
    "    given lines from file (after selecting core) determine current page number\n",
    "    \"\"\"\n",
    "    # keep reading through lines until you find a page number\n",
    "    # this will be the page number of the second page in lines\n",
    "    # so subtract 1 to get page number of current page\n",
    "    for line in lines:\n",
    "        page_number = is_page_number(line)\n",
    "        if page_number:\n",
    "            return page_number - 1\n",
    "\n",
    "\n",
    "def split_into_num_text(line):\n",
    "    \"\"\"\n",
    "    given line, split it into line number and the text.\n",
    "    \"\"\"\n",
    "    pat = r\"^(\\d*:)?(\\d+) +(.*)$\"\n",
    "    match = re.match(pat, line)\n",
    "    if match:\n",
    "        return int(match.groups()[1]), match.groups()[2]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_start_of_side_chat(text):\n",
    "    \"\"\"\n",
    "    given text, determine if it is first line of 'side chat'\n",
    "    e.g. the second line in following:\n",
    "    Q.  Doesn't this document imply x did y?\n",
    "         MR. SMITH: Objection form.\n",
    "    if yes, return the person speaking\n",
    "    if not, return False\n",
    "    \"\"\"\n",
    "    # this pattern is known to match too many things, e.g. WITNESS NAME:\n",
    "    pat = r\"^([A-Z]+\\.?( [A-Z]+)+): +[\\w\\(-]\"\n",
    "    match = re.match(pat, text)\n",
    "\n",
    "    if match:\n",
    "        return match.groups()[0]\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_start_of_brackets(text):\n",
    "    \"\"\"\n",
    "    given text, determine it is start of text that is contained in brackets\n",
    "    \"\"\"\n",
    "    pat = r\"^\\([^\\(\\)]*\\)?$\"\n",
    "    match = re.match(pat, text)\n",
    "\n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_only_symbols(text):\n",
    "    \"\"\"\n",
    "    given string, determine if it is made up of only symbols\n",
    "    intention is to find things like linebreaks '- - -'\n",
    "    \"\"\"\n",
    "    pat = \"[a-zA-Z0-9]\"\n",
    "    match = re.search(pat, text)\n",
    "\n",
    "    if match:\n",
    "        return False\n",
    "    # -- is used to indicate somebody was interrupted just before they were going to speak\n",
    "    elif text == \"--\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def create_dataframe_from_file(filename):\n",
    "    \"\"\"\n",
    "    given a deposition in filename, create a dataframe\n",
    "    \"\"\"\n",
    "    with open(dir_name + filename, \"r\", encoding=\"windows-1252\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    lines = strip_lines(lines)\n",
    "    lines = delete_blank_lines(lines)\n",
    "    lines = remove_time_tags(lines)\n",
    "\n",
    "    return create_dataframe_from_lines(lines)\n",
    "\n",
    "\n",
    "def create_dataframe_from_lines(lines):\n",
    "    \"\"\"\n",
    "    this is the most important function in preprocessing. it loops through all the lines,\n",
    "    checks what kind of line it is, and combines that line with previous lines as appropriate.\n",
    "    then outputs a dataframe\n",
    "    \"\"\"\n",
    "    # choose starting point as 5 lines above first question. this is because the questioner is almost always\n",
    "    # identified within the few lines before first question\n",
    "    start = find_first_question(lines) - 5\n",
    "    if start is None:\n",
    "        print(\"could not find start\")\n",
    "        return None\n",
    "\n",
    "    # initialize various parameters\n",
    "    ongoing_indice = 0\n",
    "    current_page_number = find_current_page_number(lines[start:])\n",
    "    current_line_number = 0\n",
    "    ongoing_page_number = 0\n",
    "    ongoing_line_number = 0\n",
    "    ongoing_text = \"\"\n",
    "    ongoing_line_type = None\n",
    "    ongoing_questioner = None\n",
    "    ongoing_speaker = None\n",
    "\n",
    "    # the indice and time_added columns are included to help with debugging. they have no use for end-users\n",
    "    columns = [\n",
    "        \"indice\",\n",
    "        \"page_number\",\n",
    "        \"line_number\",\n",
    "        \"text\",\n",
    "        \"text_type\",\n",
    "        \"speaker\",\n",
    "        \"time_added\",\n",
    "    ]\n",
    "    data = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        # ignore preamble\n",
    "        if i < start:\n",
    "            continue\n",
    "\n",
    "        # page_numbering is now determined by using line numbers and first page number\n",
    "        # see few lines below\n",
    "        if is_page_number(line):\n",
    "            continue\n",
    "\n",
    "        splits = split_into_num_text(line)\n",
    "        if splits is None:\n",
    "            continue\n",
    "\n",
    "        line_number, text = splits\n",
    "\n",
    "        if line_number < current_line_number:\n",
    "            current_page_number += 1\n",
    "        current_line_number = line_number\n",
    "\n",
    "        if is_start_of_question(text):\n",
    "            data.append(\n",
    "                [\n",
    "                    ongoing_indice,\n",
    "                    ongoing_page_number,\n",
    "                    ongoing_line_number,\n",
    "                    ongoing_text,\n",
    "                    ongoing_line_type,\n",
    "                    ongoing_speaker,\n",
    "                    \"start_question\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ongoing_indice = i\n",
    "            ongoing_page_number = current_page_number\n",
    "            ongoing_line_number = current_line_number\n",
    "            ongoing_text = text\n",
    "            ongoing_line_type = \"q\"\n",
    "\n",
    "            questioner = is_identifying_questioner(text)\n",
    "            if questioner:\n",
    "                ongoing_questioner = questioner\n",
    "                # include something here to remove the identification of questioner from question\n",
    "            ongoing_speaker = ongoing_questioner\n",
    "\n",
    "        elif is_start_of_answer(text):\n",
    "            data.append(\n",
    "                [\n",
    "                    ongoing_indice,\n",
    "                    ongoing_page_number,\n",
    "                    ongoing_line_number,\n",
    "                    ongoing_text,\n",
    "                    ongoing_line_type,\n",
    "                    ongoing_speaker,\n",
    "                    \"start_answer\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ongoing_indice = i\n",
    "            ongoing_page_number = current_page_number\n",
    "            ongoing_line_number = current_line_number\n",
    "            ongoing_text = text\n",
    "            ongoing_line_type = \"a\"\n",
    "            ongoing_speaker = \"THE WITNESS\"\n",
    "\n",
    "        elif is_start_of_side_chat(text):\n",
    "            data.append(\n",
    "                [\n",
    "                    ongoing_indice,\n",
    "                    ongoing_page_number,\n",
    "                    ongoing_line_number,\n",
    "                    ongoing_text,\n",
    "                    ongoing_line_type,\n",
    "                    ongoing_speaker,\n",
    "                    \"start_chat\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ongoing_indice = i\n",
    "            ongoing_page_number = current_page_number\n",
    "            ongoing_line_number = current_line_number\n",
    "            ongoing_text = text\n",
    "            ongoing_line_type = \"side_chat\"\n",
    "            ongoing_speaker = is_start_of_side_chat(text)\n",
    "\n",
    "        elif is_identifying_questioner(text):\n",
    "            ongoing_questioner = is_identifying_questioner(text)\n",
    "\n",
    "        elif is_start_of_brackets(text):\n",
    "            data.append(\n",
    "                [\n",
    "                    ongoing_indice,\n",
    "                    ongoing_page_number,\n",
    "                    ongoing_line_number,\n",
    "                    ongoing_text,\n",
    "                    ongoing_line_type,\n",
    "                    ongoing_speaker,\n",
    "                    \"is brackets\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ongoing_indice = i\n",
    "            ongoing_page_number = current_page_number\n",
    "            ongoing_line_number = current_line_number\n",
    "            ongoing_text = text\n",
    "            ongoing_line_type = \"brackets\"\n",
    "            ongoing_speaker = None\n",
    "\n",
    "        elif is_only_symbols(text):\n",
    "            data.append(\n",
    "                [\n",
    "                    ongoing_indice,\n",
    "                    ongoing_page_number,\n",
    "                    ongoing_line_number,\n",
    "                    ongoing_text,\n",
    "                    ongoing_line_type,\n",
    "                    ongoing_speaker,\n",
    "                    \"symbols\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            ongoing_indice = i\n",
    "            ongoing_page_number = current_page_number\n",
    "            ongoing_line_number = current_line_number\n",
    "            ongoing_text = text\n",
    "            ongoing_line_type = \"symbols\"\n",
    "            ongoing_speaker = None\n",
    "\n",
    "        else:\n",
    "            ongoing_text += \" \" + text\n",
    "\n",
    "    data.append(\n",
    "        [\n",
    "            ongoing_indice,\n",
    "            ongoing_page_number,\n",
    "            ongoing_line_number,\n",
    "            ongoing_text,\n",
    "            ongoing_line_type,\n",
    "            ongoing_speaker,\n",
    "            \"end\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(data, columns=columns)\n",
    "\n",
    "\n",
    "def remove_a_q_from_text(text):\n",
    "    \"\"\"\n",
    "    given a string, remove 'A. ' and 'Q. ' at the start of the string\n",
    "    \"\"\"\n",
    "    pat = r\"^[A|Q]\\.? +(.*$)\"\n",
    "    match = re.match(pat, text)\n",
    "\n",
    "    if match:\n",
    "        return match.groups()[0]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def remove_a_q_from_text_in_frame(df):\n",
    "    \"\"\"\n",
    "    given dataframe outputted from 'create_dataframe_from...',\n",
    "    remove the 'A. ' and 'Q. ' at the beginning of texts of type 'a' or 'q'\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    aq_indices = df_temp.text_type.isin([\"a\", \"q\"])\n",
    "    df_temp.loc[aq_indices, \"text\"] = df_temp.loc[aq_indices, \"text\"].map(\n",
    "        remove_a_q_from_text\n",
    "    )\n",
    "    return df_temp\n",
    "\n",
    "\n",
    "def remove_names_from_sidechat_text(df):\n",
    "    \"\"\"\n",
    "    given dataframe outputted from 'create_dataframe_from...',\n",
    "    remove the 'MR SMITH: ' or 'THE WITNESS: ' or similar\n",
    "    from start of side chat\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "\n",
    "    indices = df_temp.text_type == \"side_chat\"\n",
    "\n",
    "    df_temp.loc[indices, \"text\"] = (\n",
    "        df_temp.loc[indices, \"text\"].str.split(pat=\": +\").map(lambda x: x[1])\n",
    "    )\n",
    "\n",
    "    return df_temp\n",
    "\n",
    "\n",
    "def create_csvs_from_directory(directory):\n",
    "    \"\"\"\n",
    "    given a directory of text files of depositions,\n",
    "    create csv files that extract data from them\n",
    "    \"\"\"\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        print(f\"starting on {filename}\")\n",
    "        df = create_dataframe_from_file(filename)\n",
    "        df = remove_a_q_from_text_in_frame(df)\n",
    "        df = remove_names_from_sidechat_text(df)\n",
    "        df.to_csv(dir_name_csv + filename[:-4] + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## copy files from data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../data_raw/*.txt ../data_preprocessed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace spaces with underscores in filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_space_with_underscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manually remove files that aren't depositions\n",
    "files discovered not to be depositions by manual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    \"RT091119-0823_xf.txt\",\n",
    "    \"082919_DR_YARUS1.txt\",\n",
    "    \"2019-75933.txt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    path = dir_name+filename\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deleted_manual.txt\", \"w\") as f:\n",
    "    for file in filenames:\n",
    "        f.write(file + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identify bad files and delete from preprocessed folder\n",
    "A file is considered bad if the function find_first_question returns false, i.e. if there does not seem to be any questions in the file. Manually checking suggests these files are not depositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_files = find_bad_files(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in bad_files:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually view some lines from the bad files to check they are not depositions\n",
    "for filename in bad_files:\n",
    "    with open(dir_name + filename, \"r\", encoding=\"windows-1252\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = strip_lines(lines)\n",
    "    lines = delete_blank_lines(lines)\n",
    "\n",
    "    print(filename)\n",
    "    for line in lines[:400]:\n",
    "        print(line)\n",
    "    print((\"=\" * 50 + \"\\n\") * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in bad_files:\n",
    "    path = dir_name + filename\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deleted_bad.txt\", \"w\") as f:\n",
    "    for file in bad_files:\n",
    "        f.write(file + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create csv files from depositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csvs_from_directory(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove duplicate files\n",
    "this is done *after* preprocessing because some deposition files are different before preprocessing but the same after proprocessing. For example, have the same deposition with and without time tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = {}\n",
    "for filename in os.listdir(dir_name_csv):\n",
    "    filehash = hashlib.md5(open(dir_name_csv + filename, \"rb\").read()).hexdigest()\n",
    "    if filehash not in hashes:\n",
    "        hashes[filehash] = [filename]\n",
    "    else:\n",
    "        hashes[filehash].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = []\n",
    "for value in hashes.values():\n",
    "    if len(value) > 1:\n",
    "        duplicates.append(value)\n",
    "\n",
    "for i in duplicates:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that files identified as duplicates by hashing are indeed duplicates\n",
    "# by manually viewing excerpts from them\n",
    "\n",
    "for duplicate in duplicates:\n",
    "    for filename in duplicate:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(dir_name_csv + filename)\n",
    "        print(df.head(10).text)\n",
    "        print(\"=\" * 10)\n",
    "\n",
    "    print((\"=\" * 50 + \"\\n\") * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_files = []\n",
    "\n",
    "for duplicate in duplicates:\n",
    "    for filename in duplicate[1:]:\n",
    "        deleted_files.append(filename)\n",
    "        path = dir_name_csv + filename\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "        path = dir_name + filename[:-3] + \".txt\"\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deleted_duplicates.txt\", \"w\") as f:\n",
    "    for file in deleted_files:\n",
    "        f.write(file + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing functions on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(dir_name):\n",
    "    #     print(filename)\n",
    "\n",
    "    with open(dir_name + filename, \"r\", encoding=\"windows-1252\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    lines = strip_lines(lines)\n",
    "    lines = delete_blank_lines(lines)\n",
    "    lines = remove_time_tags(lines)\n",
    "\n",
    "    found = find_first_question(lines)\n",
    "\n",
    "    if not found:\n",
    "        print(\"=\" * 10)\n",
    "        print(filename)\n",
    "        print(\"=\" * 10)\n",
    "    elif not is_identifying_questioner(lines[found - 1]):\n",
    "        print(filename)\n",
    "        for line in lines[found - 3 : found + 1]:\n",
    "            print(line)\n",
    "        print(\"\\n\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing functions on individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = filenames[64]\n",
    "# filename = \"Morton,_David_-_Vol._1.txt\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_name + filename, \"r\", encoding=\"windows-1252\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = strip_lines(lines)\n",
    "lines = delete_blank_lines(lines)\n",
    "lines = remove_time_tags(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe_from_lines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_name + filename, \"r\", encoding=\"windows-1252\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines = strip_lines(lines)\n",
    "lines = delete_blank_lines(lines)\n",
    "lines = remove_time_tags(lines)\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "#     print(line)\n",
    "    num_text = split_into_num_text(line)\n",
    "\n",
    "    if num_text is None:\n",
    "        continue\n",
    "    if is_identifying_questioner(num_text[1]):\n",
    "        print(is_identifying_questioner(num_text[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tracking structure of files\n",
    "I skimmed through several depositions to find out what structure they have in common, to help determine how to carry out the preprocessing\n",
    "\n",
    "08-09_Telesforo_Camacho-Lopez.txt\n",
    "* Q and A. dots, many spaces\n",
    "* page numbers. \n",
    "* start of dep. BY MS. NOSARI:\n",
    "* side chats. THE INTERPRETER: ... \n",
    "* questions restarting after side chat. BY ...:\n",
    "* questions restarting after misc.\n",
    "* misc. - - - \\n stuff - - - \n",
    "\n",
    "* Q and A.\n",
    "* page numbers. \n",
    "* start of dep. \n",
    "* side chats.\n",
    "* questions restarting after side chat.\n",
    "* questions restarting after misc.\n",
    "* misc\n",
    "\n",
    "\n",
    "RT091119-0823_xf.txt. \n",
    "* Q and A. no dots, many spaces. spacing between Q and A are different\n",
    "* page numbers. \n",
    "* start of dep. EXAMINATION BY MR. HELLER: (no new line!)\n",
    "* side chats. ARBITRATOR COHEN\n",
    "* questions restarting after side chat. BY MR. HARRISON:\n",
    "* questions restarting after misc. BY MR. HARRISON:\n",
    "* misc. Has headings. 'A. Qureshi - Heller'\n",
    "* new lawyer, EXAMINATION \\n BY MR. HARRISON:\n",
    "* there are multiple witnesses..\n",
    "* Z A I N A B ... being first duly affirmed by notary....   new witness questions start as normal. EXAMINATION \\n BY..\n",
    "* this is not a deposition!! deleted\n",
    "\n",
    "\n",
    "Trudnak_and_Rancourt_v._Barth_-_8-27-19_-_Christian_Pizarro_M.D._-_FINAL.txt\n",
    "* Q and A. dots, sevreal spaces.\n",
    "* page numbers. yes\n",
    "* start of dep. EXAMINATION \\n BY MR. ...:\n",
    "* side chats. MR. ... :\n",
    "* questions restarting after side chat.\n",
    "* questions restarting after misc.\n",
    "* misc\n",
    "\n",
    "8-20-19-TS.txt\n",
    "* Q and A. Q. and A. with two spaces\n",
    "* page numbers.\n",
    "* start of dep. EXAMINATION \\n BY MR. ELLERBE:\n",
    "* side chats. THE VIDEOGRAPHER: MR. ...: \n",
    "* questions restarting after side chat. BY MR. ELLERBE:\n",
    "* questions restarting after misc.\n",
    "* misc. sometimes a question is answered in the side chat...\n",
    "* misc. has long answers, with questioning saying OK in between to confirm their understanding...\n",
    "* misc. marking exhibit done in brackets, then questions restarted with BY MR. ...:\n",
    "* (reviewing documents) has same spacing as answers? less spaces than (exhibit x marked)\n",
    "* has time tags that are only 1 space away from text...\n",
    "* restarting after break. EXAMINATION \\n BY MR. ROBINSON\n",
    "* FURTHER EXAMINATION \\n BY MR. ELL...:\n",
    "\n",
    "Collins,_Mary_-_Vol._1.txt\n",
    "* Q and A. with dots and multiple spaces\n",
    "* page numbers.\n",
    "* start of dep. EXAMINATION CONDUCTED \\n BY MR. SUGARMAN:\n",
    "* side chats. MR. SUGARMAN: THE WITNESS:  MR. DEATON:  more spaces than start of q and a, but same spacing as paragraphs within a q and a.\n",
    "* questions restarting after side chat. nothing. just Q.   \n",
    "* questions restarting after misc.\n",
    "* misc.  separating sections. * * * * *\n",
    "* misc. examined by 5 people\n",
    "* misc. 'strike that'\n",
    "* misc. had question, then side-chat, then question. so a question without an answer.\n",
    "* misc. new lawyer.  EXAMINATION CONDUCTED \\n BY MR. AGUDELO:\n",
    "* misc. new lawyer.  EXAMINATION CONDUCTED \\n  BY MS. VELLUCCI:\n",
    "* marking exhibit done in brackets. Q.   But we will mark it as Exhibit 2. \\n (Exhibit 2, Bottle of Powder, so\\n marked.) \\n   Q.   Now, a couple of follow-ups, ma'am,\n",
    "* misc. A doesn't understand, in side chat, Q asks for testimony to be read. then in brackets we have (statement read out), then A continues to answer\n",
    "\n",
    "\n",
    "\n",
    "Capitol_Indemnity_Corporation_v._Euro_Motorcars_Devon_-_8-28-19_-_Smith_-_FINAL.txt\n",
    "* Q and A. without dots. Multiple spaces.\n",
    "* Page numbers. top lines of each page\n",
    "* start of dep. BY MR. BLUM:\n",
    "* side chats. extra spaces. MR. GOGINENI:... THE WITNESS:... MR. BLUM: ...\n",
    "* interupptions. your -- ... -- landlord?\n",
    "* interruptions. left was cinder --.  A  concrete, cinderblock.  Q   cinderblock, okay. Where there any. (no -- on restart)\n",
    "* misc. - - - (new line) exhibit (new line) - - -\n",
    "* misc. can have -- in middle of sentence, not indicating interruptions, but pauses\n",
    "* questions restarting after side chat. BY MR. BLUM:\n",
    "* questions restarting after other. BY MR. BLUM:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
